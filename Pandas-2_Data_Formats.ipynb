{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![Pandas logo](img/pandas.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data\n",
    "\n",
    "Pandas comes with I/O functions for a great many different formats.  You can get a feel for most of them with the Jupyter interactive help, by typing `pd.read_<tab>` to get a pop-up showing the functions with that prefix.  You can get a similar list of built-in output formats once you have loaded a DataFrame by typing `df.to_<tab>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partial list of formats Pandas supports are:\n",
    "\n",
    "- [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)\n",
    "- [Excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html)\n",
    "- [SQL](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)\n",
    "- [JSON](http://www.json.org/)\n",
    "- [HDF5](https://www.hdfgroup.org/HDF5/)\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html)\n",
    "- [msgpack](http://msgpack.org/)\n",
    "- [Stata](https://en.wikipedia.org/wiki/Stata)\n",
    "- [Apache Parquet](https://parquet.apache.org/documentation/latest/)\n",
    "- [Google BigQuery](https://en.wikipedia.org/wiki/BigQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV\n",
    "\n",
    "Delimited files—most commonly comma-separated—are a very common data format for shared data.  Pandas' `.read_csv()` function has a huge number of optional parameters to control how a data file is read into a DataFrame.  One obvious and important one is `sep=<delimiter>` which defaults to comma but could be any other character.\n",
    "\n",
    "Let us look at a few options against a small file we saw earlier.  Run `pd.read_csv?` in a cell to get a lot of documentation of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While CSV is widely used and even reasonably fast to read, it does have some limitations.  Values within CSV or similar text formats are not typed directly in the encoding.  We need either to rely on our tool, e.g. Pandas, to infer types for us; or we need to specify them explicitly on import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/patient-records.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `.info()` display, strings are generically Python objects.  For a person name, that is the best choice, but the date should be handled more specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the date in the \"date colum\"\n",
    "df = pd.read_csv('data/patient-records.csv', parse_dates=['date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names, explicit types of some columns, choose index\n",
    "patients = pd.read_csv('data/patient-records.csv', \n",
    "                       skiprows=1,\n",
    "                       names=['Patient', 'Visit_Date', 'Weight', 'Height'],\n",
    "                       parse_dates=['Visit_Date'],\n",
    "                       dtype={'Height': np.float16, 'Weight': np.float16},\n",
    "                       index_col='Visit_Date')\n",
    "patients                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns as Attributes\n",
    "\n",
    "This simple DataFrame is an oportunity to note that if we happen to have column names that are valid Python identifiers, we can use a more compact attribute-style access rather than the dictionary-style indexing by column.  The square brackets are perfectly general, but the dot style sometimes looks nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients[(patients.Height > 165) & (patients.Weight < 80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients[(patients['Height'] > 165) & (patients['Weight'] < 80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in a while you can get in trouble with the dot access because some attribute is both a column name and a DataFrame method or standard attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"A\": [1, 5, 7], 'std': [31, 52, 68]})\n",
    "df.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a DataFrame Back to CSV\n",
    "\n",
    "After we have manipulated DataFrames in some manner, we can easily write them to new delimited files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "patients.to_csv('tmp/patients.txt', sep='|', quoting=csv.QUOTE_NONNUMERIC)\n",
    "!cat tmp/patients.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "There are a couple ways we can interact with SQL databases.  In particular, we can either read entire tables directly, or we can read the results of arbitrarily comple queries into DataFrames.  For the illustration here, an SQLite3 database is provided in the training materials.  However, the identical interfaces would work connecting to a distributed RDBMS (getting credentials correct for the connection object is a different topic, but once connection is established it is the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we first read in the Wisconsin breast cancer data set from CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_csv = pd.read_csv('data/wisconsin.csv')\n",
    "cancer_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the RDBMS we use, the data is broken out into separate tables for the features and the target.  In a more real world example, we would generally encounter many tables with a variety of foreign key constraints, triggers, indices, and so on.  Using some plain SQL (no Pandas), let us take a look at the schemata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import sqlite3\n",
    "db = sqlite3.connect('data/wisconsin.db')\n",
    "cur = db.cursor()\n",
    "cur.execute(\"SELECT * FROM sqlite_master;\")\n",
    "pprint(cur.fetchall())\n",
    "del db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire table\n",
    "\n",
    "Unfortunately, Pandas often relies on the SQLAlchemy *object relational mapper* (ORM) rather than use the raw database connection.  Usually the wrapper does no harm, but ORMs *do impose* their world view too much in much of their API (not the parts used here, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///data/wisconsin.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_features_sql = pd.read_sql_table('features', engine, \n",
    "                                        index_col='Observation_No')\n",
    "cancer_features_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on a query\n",
    "\n",
    "We can issue an arbitrary query rather than read an entire table into a DataFrame.  If memory permits, it is often friendlier to do filtering and other operations within Pandas rather than within SQL; but this style allows whichever combination is most useful to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `pd.read_sql()` is polymorphic in taking either a table name or a query, then doing some basic pattern matching to figure out which kind of request it is.  We find it better practice to be explicit about the query involved though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT f.Observation_No, f.mean_radius, f.mean_texture, \n",
    "       f.mean_perimeter, t.benign \n",
    "FROM features f \n",
    "INNER JOIN target t \n",
    "WHERE f.Observation_No = t.Observation_No\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "cancer_with_target_sql = pd.read_sql_query(sql, engine, \n",
    "                                           index_col='Observation_No')\n",
    "cancer_with_target_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a DataFrame back to a table\n",
    "\n",
    "After we have manipulated DataFrames in some manner, we can easily write them to tables in our RDBMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm tmp/cancer-partial.sqlite\n",
    "db = sqlite3.connect('tmp/cancer-partial.sqlite')\n",
    "cancer_with_target_sql.to_sql('my_table', db)\n",
    "\n",
    "pprint(db.execute(\"SELECT * FROM my_table LIMIT 5\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel Spreadsheets\n",
    "\n",
    "Unfortunately, a great deal of data in the world lives in spreadsheets, and in Microsoft Excel specifically.  There are many things bad and broken about this format, but data comes as we get it.  For example:\n",
    "\n",
    "* Very unreliable datatyping of columns and cells, especially if human entered data\n",
    "* Poor handling of missing values\n",
    "* Ragged, sparse, and irregular use of rectangular grid of rows/columns\n",
    "* Computation expressed in distributed and hard-to-trace manner\n",
    "* Inconsistent uses of tabs for hierarchy or structure\n",
    "* Limitation on number of rows and inflexible datatypes\n",
    "* Mediocre heuristics for inference of datatypes\n",
    "* Use of columns for values of inconsistent datatypes\n",
    "* **Extremely** slow compared to nearly every data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows us to read Excel sheets, and even to clean up the bad data we find once they are loaded into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_excel = pd.read_excel('data/wisconsin.xlsx', \n",
    "                             sheet_name=['Features', 'Target'],\n",
    "                             index_col='Observation_No')\n",
    "cancer_excel['Features'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_excel['Target'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Data\n",
    "\n",
    "Many of the `.read_<format>()` methods in Pandas are happy to accept URLs following a number of schemata (HTTP, HTTPS, FTP, S3, file, and others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = (\"https://bitbucket.org/davidmertz/sample-data/\"\n",
    "       \"raw/9fb79b1e993e58ba7e3137f63726bb4a19bc0e8e/patient-records.csv\")\n",
    "pd.read_csv(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "Pandas DataFrames are inherently two dimensional grids of values.  This is very broadly useful, and maps well to formats like CSV, spreadsheets, and relational database tables or queries.  However, other data is hierarchical or has other structures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most widely used hierarchical formats is Javascript Object Notation (JSON).  Flat grids are a subset, conceptually, of all possible nested structures.  But JSON is extermely widely used for data transmitted between web services, and for other purposes, so Pandas provides a number of ways to read/write JSON data that is \"conceptually flat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some JSON formats result in lossy round-tripping with DataFrames since they do not represent all the components of a DataFrame.  Another thing to trip over is that JSON types by values; so e.g. if all values in a column are *equal to an integer* the writer will cast as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': [1.0, 2, 3], 'col2': [5.1, 6.2, 7.3]},\n",
    "                  index=['A', 'B', 'C'])\n",
    "df.index.name = \"Letter\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'split'\n",
    "json = df.to_json(orient=style)\n",
    "pprint(json)\n",
    "pd.read_json(json, orient=style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'records'\n",
    "json = df.to_json(orient=style)\n",
    "pprint(json)\n",
    "pd.read_json(json, orient=style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'index'\n",
    "json = df.to_json(orient=style)\n",
    "pprint(json)\n",
    "pd.read_json(json, orient=style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'columns'\n",
    "json = df.to_json(orient=style)\n",
    "pprint(json)\n",
    "pd.read_json(json, orient=style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'split'\n",
    "json = df.to_json(orient=style)\n",
    "pprint(json)\n",
    "pd.read_json(json, orient=style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'table'\n",
    "json = df.to_json(orient=style)\n",
    "pprint(json)\n",
    "pd.read_json(json, orient=style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet and Arrow\n",
    "\n",
    "Pandas can also read both the Parquet and Arrow formats, which are **far** higher performance than formats like Excel, CSV, JSON, and so on.  Both of these formats are column oriented and strictly data typed, which make them well suited to working on their data in data frames.\n",
    "\n",
    "These two formats are slightly different creatures, however.  Arrow is a language-independent *memory format* so that varying libraries, across varying programming languages, can access the same data without performing copying.  For example, both Python Pandas and R dplyer can operate on the same memory, in principle.  Arrow also defines a serialization format, called Feather, which is essentially just a direct map of bytes in memory to bytes on disk.\n",
    "\n",
    "Parquet is column-oriented *data file format* designed with very efficient data compression and encoding schemes.  However, when read into memory, Parquet data simply has the native in-memory layout of your data frame library. For modern versions of Pandas, that memory layout simply *is* Arrow.\n",
    "\n",
    "In most cases, you should prefer to read and write Parquet rather than Arrow.  Its compression causes its files to use less disk space, and usually to read and write more quickly as well since CPU compression speeds greatly outpace disk write speeds.  Most of the same large range of tools, across many programming languages, support both Parquet and Arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_csv.to_parquet(\"tmp/wisconsin.parquet\")\n",
    "pd.read_parquet(\"tmp/wisconsin.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_csv.to_feather(\"tmp/wisconsin.feather\")\n",
    "pd.read_feather(\"tmp/wisconsin.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "For a first few exercises, you will read and write from a variety of formats to get a feel for them.  Moreover, the speed of different formats can be dramatically different, so time operations against different formats (both reading and writing) using the `%timeit` magic in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `%timeit` macro creates its own namespace for operations, so when you want the actual result from an operation to work with more, you will need to run the function or method outside of the macro also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Data\n",
    "\n",
    "Download the datasets provided as CSV URLs, and also save them locally as CSV.  Using the `tmp/` relative path within this training repository is a good choice of location to save files at.\n",
    "\n",
    "* Is loading from local storage faster than from remote URL?\n",
    "* What do you conclude about the main speed limitations in loading data from CSV?\n",
    "* Does the same pattern hold for the smaller \"Baby Names\" dataset and the larger \"Daily Temperatures\" dataset?\n",
    "* Were the inferred datatypes all the best choices for the respective fields?\n",
    "* If you wanted to read some fields as different datatypes, how would you do that?\n",
    "* What effect does file compression have on reading and writing CSV format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset obtained from https://www.data.gov/\n",
    "url = (\"https://bitbucket.org/davidmertz/sample-data/raw/\"\n",
    "       \"1bd7d3fbfc6842eb067ea9a9354b6b4e5b8597ab/Popular_Baby_Names.csv\")\n",
    "babynames = pd.read_csv(url)\n",
    "babynames.to_csv('tmp/babynames.csv')\n",
    "%timeit pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The United States [National Oceanic and Atmospheric Administration (NOAA)](https://www.noaa.gov/) provides a number of useful datasets.  We will work with a subset of daily temperatures worldwide during 2019, which was obtained from https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/.  Notice that Pandas will seamlessly read compressed CSV files as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"https://bitbucket.org/davidmertz/sample-data/raw/\"\n",
    "       \"61872271984f66e3094c367cf90dfc4875a22e8d/NOAA-2019-partial.csv.gz\")\n",
    "temperatures = pd.read_csv(url)\n",
    "%timeit pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Different Formats\n",
    "\n",
    "Save these datasets in other formats we have discussed, or even only mentioned in passing.  \n",
    "\n",
    "* How does the speed of reading and writing compare?\n",
    "* Do you lose any information round-tripping between various formats? (datatypes are information too)\n",
    "* How do you expect performance to scale if you were to move from half-a-million rows to a hundred million rows?\n",
    "* Would the answer vary across different formats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Advantages and Disadvantages\n",
    "\n",
    "Explain to the person sitting next to you (or write in a cell below) what advantages and disadvantages you find in using different storage formats for Pandas DataFrames.  It is fair if one of the advantages is \"my boss/work requires me to use X.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation of virtues and demerits of formats ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Data\n",
    "\n",
    "Using ideas we have presented in passing and patterns you learned working with NumPy, try to extract only the temperatures for January, February, and March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, the solstice and equinox do not line up with months exactly\n",
    "# If you would like to find the accurate cut-offs for \"winter\", please do\n",
    "winter = ... # something using `temperatures`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being data from the United States, temperatures are unfortunately provided in Fahrenheit. At least it would appear so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures['TEMP'].max(), temperatures['TEMP'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the data is a subset of the full year data. In later lessons we try to characterize the distributions of dates, longitudes, lattitudes, elevaations, and so on.  But, we do know:\n",
    "\n",
    "> The official highest recorded temperature is now 56.7℃ (134℉), which was measured on 10 July 1913 at Greenland Ranch, Death Valley, California, USA.\n",
    "\n",
    "So 92.2 would be quite low if the whole earth was represented as ℉.  But the number is vastly higher than any earth-surface ℃."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extra credit, see if you can figure out how to add a new column to the `temperatures` DataFrame called `TEMP(C)` with the obvious meaning and correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for Celcius temperatures\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your improved data back to disk in your preferred data format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
