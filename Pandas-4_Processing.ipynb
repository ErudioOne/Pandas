{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Pandas logo](img/pandas.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "The notion of applying operations in a vectorized way is common between Pandas and NumPy.  As with NumPy, vectorization goes hand-in-hand with filtering.  We usually want to apply some transformation on *many* elements, but usually not *all* the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick example, let us look at a slightly enhanced version of our patient data.  This version contains an `age` field as well as the others we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv('data/patients-with-age.csv', \n",
    "                       parse_dates=['date'])\n",
    "patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure called Body Mass Index (BMI) is sometimes used to judge overall health in a quick way, with a \"healthy range\" between 18.5 and 24.9, with underweight or overweight past those limits.  There are many caveats in using this measure medically, but the numeric formula is:\n",
    "\n",
    "$$BMI = \\frac{kg}{m^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this measure to our patients, but we feel that it is only applicable to adults over age 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi = patients['weight(kg)'] / (patients['height(cm)'] * 0.01)**2\n",
    "# Notice the access both filters rows and adds a new column\n",
    "patients.loc[patients.age >= 18, \"BMI\"] = bmi\n",
    "patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write this formula in one line, but sometimes you wish to perform a more complex calculation that you would like to encapsulate in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmi_calc(row):\n",
    "    kg = row['weight(kg)']\n",
    "    m = row['height(cm)'] / 100\n",
    "    return kg / m**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis 1 is row-by-row (axis 0 is column-by-column)\n",
    "patients.apply(bmi_calc, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Dates\n",
    "\n",
    "Our simple `patients` DataFrame contains several numeric columns, but it also contains a datetime column and string column.  Those have some special \"accessors\" of their own.  A quirk of Pandas and Python is that even though Pandas knows a column is a special type, you are required to call type-specific methods via these accessors (there are obscure reasons for this relating to Python's runtime model; just take it as given)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we would like to maniuplate or filter dates in some way that is aware of their decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.date.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients who visited on a weekend\n",
    "patients[patients.date.dt.day_name().isin(['Saturday', 'Sunday'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a great deal you can do with datetime accessors, such as accessing the hour, minute, second, or microsecond of the timestamp (where those are used, unlike in this example).  A later module will look at working with time series in much more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Strings\n",
    "\n",
    "Another special kind of data you often deal with is strings.  Usually this is encoded generically as Python objects internally.  But reading from most kinds of data sources will make those objects consistently strings when that is the kind of field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with datetimes, there are many non-numeric things you often want to do with strings.  Basically all the capabilities of Python string methods—and some additional ones—are provided in vectorized versions for Pandas Series (i.e. string columns of DataFrames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = patients.copy()\n",
    "# Set the index to the name column\n",
    "df.index = df.name\n",
    "print(\"How many 'a's are in each patient's name?\")\n",
    "df.name.str.count('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could solve an earlier problem of using uppercase name for the index in pure-Pandas style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.name.str.upper()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completely silly, but replace the a's with a-ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name.str.replace('a', 'å')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While single single first names are a bit too short and simple for this to be useful, we can split strings in a vectorized way.  For things we may often encounter like structured or delimited strings, this can be extremely powerful.  Moreover, this split operation is a full regular-expression pattern split, not simply a split on a delimeter only; in concept we can divide strings into parts in extermely powerful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep in mind though is that the default operation mode produces a Series of Python lists with the split components.  This is often not the most useful result if the splits represent derived \"features\" of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name.str.split('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often what you would like instead is to create multiple columns of derived \"data\" based on the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substrings = df.name.str.split(r'[Aa]', expand=True)\n",
    "substrings.columns = ['part1', 'part2', 'part3']\n",
    "substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've been discussing strings and `.str` methods, `.join()` should here be read strictly in the sense of SQL and relational algebra (not as `str.join()` or a cousin).  That is, it's basically `JOIN df, substrings ON df.index = substrings.index` for folks who know SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(substrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises here are about **cleaning data**.  I happen to have written a very good book about exactly this topic, largely using Pandas to explore the topic.  Readers might want to check out [_Cleaning Data for Effective Data Science_](https://gnosis.cx/cleaning/), David Mertz, Ph.D. ISBN-13 978-1801071291, 30 March 2021.\n",
    "\n",
    "I'd love it if you want to buy a copy, but you can read it freely online as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read in the moderately large NOAA temperature dataset we worked with before.  Our naïve manner of reading it simply allows Pandas to guess types.  For the most part that works well, but there is a date field we want to go back and adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"https://bitbucket.org/davidmertz/sample-data/raw/\"\n",
    "       \"61872271984f66e3094c367cf90dfc4875a22e8d/NOAA-2019-partial.csv.gz\")\n",
    "temperatures = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with show_all_rows():\n",
    "    print(temperatures.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same column name, but do a vectorized conversion to datetime\n",
    "# Note also that we can *access* the column in attribute style,\n",
    "#  ... but we can only *set* it using the dictionary style\n",
    "temperatures['DATE'] = pd.to_datetime(temperatures.DATE)\n",
    "temperatures.dtypes.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fields\n",
    "\n",
    "A description of this data set can be found at: [Global Surface Summary of the Day](https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.ncdc:C00516).  This gives details on *some* of the fields in the data, but is a bit incomplete.  NOAA is a wonderful agency, but data is always dirty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, being a United States agency, most of the data are in Imperial Units, and you probably want SI units to make sense of the data.  We would like to create a new DataFrame called `temperatures_SI` that contains the data in `temperatures`, but using better units:\n",
    "\n",
    "* Mean temperature: Fahrenheit → Celcius (or Kelvin)\n",
    "* Mean dew point: Fahrenheit → Celcius (or Kelvin)\n",
    "* Mean sea level pressure: millibars → hectopascals\n",
    "* Mean station pressure millibars → hectopascals\n",
    "* Mean visibility: miles → kilometers\n",
    "* Mean wind speed: 1 knots → m/s\n",
    "* Maximum sustained wind speed: knots → m/s\n",
    "* Maximum wind gust: knots → m/s\n",
    "* Maximum temperature: Fahrenheit → Celcius (or Kelvin)\n",
    "* Minimum temperature: Fahrenheit → Celcius (or Kelvin)\n",
    "* Precipitation amount: inches → centimeters\n",
    "* Snow depth inches → centimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversions here\n",
    "temperatures_SI = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporing the data\n",
    "\n",
    "While the column names provided are *suggestive* of their full names given in the metadata description, often very abbreviated names are used.  Try to verify using the data itself that you have made the right assumptions about the field meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can be pretty confident that the *maximum* of something should almost always be more than the *minimum* of that same thing.  However, datasets are always dirty, so *almost always* may have to suffice for this verification if *always* does not apply.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e. if `FOO` values are typically in the range of 10-20, if you see a value of 1,000,000 it is almost surely a data error rather than an unusual fluctuation in the underlying phenomenon.  Whether it is an error in collection, transcription, conversion, or of some other kind, is much harder to determine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of dataset:\", len(temperatures))\n",
    "print(\"MAX more than MIN\", (temperatures.MAX > temperatures.MIN).sum())\n",
    "print(\"MIN more than MAX\", (temperatures.MIN > temperatures.MAX).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, although your answer may not be definitive, try to characterize the nature of the problem data.  Do so both for the above provided example, and then for other abnormalities you detect in your own valuidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform more data validation\n",
    "...\n",
    "# As you perform this, describe to your neighbor or in this notebook\n",
    "# what expectations you have for analyses and what the results are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undocumented data field\n",
    "\n",
    "**Extra Credit**: In writing this material, the units used for `ELEVATION` do not seem to be documented at the metadata description page.  Can you determine the units based on other data you have available? The most likely units seem to be feet or meters (where 1 meter is approxmiately 3.28 feet; and in the USA, elevations are most often given in feet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduce units for a field\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for scientific hypothesis\n",
    "\n",
    "Let us state a silly and simplified scientific goal.  The point here to practice Pandas, not to do actual meteorology or climate modeling. We have a theory that the underlying temperature inside the Arctic Circle would be 5% higher (in Celcius degrees) if not for wintertime albedo.  Pretend winter means January/February/March for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame `tempeartures_theoretical` based on `temperatures_SI` in which all temperatures have been adjusted to match our model.  \n",
    "\n",
    "You can check your work, in part by making sure that 3.14% of the measurements will be adjusted in this exercise.  The fact that the target ratio is very close to pi is either a curious coincidence or of deep numerological significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust arctic tempeartures in winter by 5% ℃\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
