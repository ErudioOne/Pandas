{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Erudio logo](img/erudio-logo-small.png)\n",
    "---\n",
    "![Pandas logo](img/pandas-logo-small.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "\n",
    "One of the most flexible things in Pandas is its handling of time series data.  We have seen some of that already in the `.dt` accessor for datetime columns.  It gives us quite a bit more, and it does it across many time scales.  I.e. it will work well with yearly observations of eons, and it will work well with microscond observations over a millisecond of some short event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will look at a subset of the NOAA data again.  This time we will look at just one weater station to focus on the time series aspect only.  The fields that are therefore the same for every row are pulled out, and are these values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "NAME         VERLEGENHUKEN, NO\n",
    "STATION             1002099999\n",
    "LATITUDE                 80.05\n",
    "LONGITUDE                16.25\n",
    "ELEVATION                    8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What remains is only fields that are variable with date, or at least that can be.  Moreover, the data you will read in has no meaningful order, although it was deterministically \"randomized\" based on the data (*Extra bonus credit to any student who figures out the original order*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/verlegenhuken.csv', parse_dates=['DATE'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Index\n",
    "\n",
    "Very often the most useful way to treat a date column is by making it the index column.  Moroever, in time series data, we almost always want to treat it in sequential order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('DATE')\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Index Values\n",
    "\n",
    "There is a problem that may not be immediately obvious.  The periodicity of the data is generally daily measurements, but some are missing.  We can get a hint about this by noticing there are 344 rows in our DataFrame.  But comparing the now sorted date index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have done same with the regular DATE column\n",
    "df.index[-1] - df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternately, if perhaps not sorted (same for regular column)\n",
    "df.index.max() - df.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many analyses we want regular intervals in our data.  Pandas lets us fill in our DataFrame to match whatever frequency we would like.  Notice that January 2 is added, although so far with NaNs for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has 354, not 353, rows because of \"fence posts\"\n",
    "df.asfreq('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make sense for this example, but other frequencies are equally possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT modify the original DataFrame, just demonstrating\n",
    "df.copy().asfreq('6h').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT modify the original DataFrame, just demonstrating\n",
    "df.asfreq('5d').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the daily sampling now\n",
    "df = df.asfreq('1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Values\n",
    "\n",
    "So far we just have `NaN` values filling in all the features for the added days.  That is not wrong, necessarily, but we might want to impute some values for that missing data in order to fit some smooth model we are working with.  Obviouly, invented values are not true observations, but they are often useful if used with awareness of this limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main techniques for imputing values:\n",
    "\n",
    "* Forward fill: take the prior value and assume the next is the same\n",
    "* Backward fill: take the subsequent value and use that where missing\n",
    "* Interpolation: Use some functional form to impute values based on others in the series\n",
    "  * The default and far most common technique is `linear` which just averages adjacent values\n",
    "  * The `time` option can be useful if your datetime index is not regularized; essentially it is linear if the missing rows *had been* added\n",
    "  * A number of other more complex curve fitting techniques are available, but much more specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Jan 2 example\n",
    "df.loc[:, ['TEMP', 'DEWP']].ffill().head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Jan 2 example\n",
    "df.loc[:, ['TEMP', 'DEWP']].bfill().head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Jan 2 example\n",
    "# \"linear\" is default if not specified\n",
    "df = df.interpolate(method=\"linear\")\n",
    "df.loc[:, ['TEMP', 'DEWP']].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Although it goes by a different name, resampling is essentially the same thing as `.groupby()`.  The difference is that the groups are defined by time periods.  In principle, we could achieve the same effect by creating a synthetic feature for the time period, but this use is common enough that a shortcut is much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average temperature and dew point in each 2 week window\n",
    "df.loc[:, ['TEMP', 'DEWP']].resample('2w').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum temperature and dew point in each 2 week window\n",
    "# Some aggregation is always required, as with .groupby\n",
    "df.loc[:, ['TEMP', 'DEWP']].resample('2w').max().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Windows\n",
    "\n",
    "Strictly speaking, rolling windows do not depend on having datetime columns, only on ordered data.  The idea is only to look at some adjacent element in whatever order the data is sorted.  However, this is particularly likely to be useful for time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rolling window is often more powerful than fixed windows.  Rather than, for example, take every two week block as a distinct group to aggregate over, we can effectively take every two week window around the current row as the aggregation unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = df.loc[:, [\"TEMP\", \"DEWP\"]].rolling(window=14).mean()\n",
    "rolling.columns = [\"roll_%s\" % col for col in rolling.columns]\n",
    "rolling.loc['2019-01-13':'2019-01-20', ['roll_TEMP', 'roll_DEWP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the rolling average temperature to the actual daily temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rolling\n",
    "     .join(df)\n",
    "     .iloc[12:26]\n",
    "     .loc[:, ['TEMP', 'roll_TEMP']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise as a plot to compare the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rolling\n",
    "     .join(df)\n",
    "     .loc[:, ['TEMP', 'roll_TEMP']]\n",
    "     .plot(title=\"Daily and smoothed temperature\")\n",
    "     .set(ylabel='Â°Fahrenheit')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "For the below exercises, we will read in the same dataset for Verlegenhuken as in this module.  You may want to perform the same or similar cleanup we did in the lessons to solve the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of these problems, it might be useful to look at more rows than the default configuration of 10.  We provide a small context manager to do this within one cell.  For example:\n",
    "\n",
    "```python\n",
    "# If new_max not given, show all possible\n",
    "with show_all_rows(new_max=100):\n",
    "    print(my_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pandas_exercises import *\n",
    "verl = pd.read_csv('data/verlegenhuken.csv', parse_dates=['DATE'], index_col='DATE')\n",
    "verl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which month had the most forceful wind gust? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify month with most foreceful gust\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followup: Can you say with certainty based on the data available? Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain answer with prose or code\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which month had the lowest typical wind speed? Again explain any caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify least windy month\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Produce a two month rolling average of the dew point and select 10-day intervals from that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The desired answer is in ex7_1.result for guidance\n",
    "ex7_1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Month by month, what was the change in atmospheric pressure between the first and last day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The desired answer is in ex7_2.result for guidance\n",
    "with show_all_rows():\n",
    "    print(ex7_2.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Materials licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) by the authors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
